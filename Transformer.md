AI的几个基本概念：
人工智能的主要子集
1.	机器学习（Machine Learning）
 - 你已经知道的，是AI的"主力军"
	包含：监督学习、无监督学习、强化学习
	深度学习是机器学习的"高级版本"
2.	自然语言处理（NLP）
 - 让AI能听懂和说人话
	例子：Siri、微信聊天机器人、翻译软件
	就像我们能理解"我想吃火锅"，AI也能理解这句话
3.	计算机视觉（CV）
 - 让AI"看"得清清楚楚
	例子：人脸识别解锁手机、自动驾驶的"眼睛"
	就像我们能认出猫的耳朵和胡须，AI也能从图片中识别
神经网络（Neural Networks）
	这是深度学习的"大脑"，是深度学习的基础
	模仿人脑神经元的工作方式
	基本结构：输入层 → 隐藏层（多层）→ 输出层
	用"权重"和"非线性函数"来处理信息
AI（人工智能）大框架
├─ 机器学习（让计算机从数据中学习）
   └─ 深度学习（使用多层神经网络的高级机器学习）
      └─ 神经网络（深度学习的核心结构）
         ├─ 卷积神经网络（CNN）- 专攻图像
         ├─ 循环神经网络（RNN）- 专攻序列数据
         └─ Transformer - 当前最火的模型

深度学习 vs 强化学习：核心区别
深度学习（DL）：
	核心：用神经网络自动提取特征
强化学习（RL）：
	核心：通过与环境交互，学习最优决策策略

计算机视觉（Computer Vision，CV）为起源发展起来的神经网络，其核心架构有三种：
	全连接神经网络（Feedforward Neural Network，FNN），即每一层的神经元都和上下两层的每一个神经元完全连接，如图2.1所示:
 
图2.1 全连接神经网络
	卷积神经网络（Convolutional Neural Network，CNN），即训练参数量远小于全连接神经网络的卷积层来进行特征提取和学习，如图2.2所示:
 
图2.2 卷积神经网络
	循环神经网络（Recurrent Neural Network，RNN），能够使用历史信息作为输入、包含环和自重复的网络，如图2.3所示:
 
如果我们要学习Transformer架构，其中最核心的就是它的注意力机制了
注意力机制最先源于计算机视觉领域，其核心思想为当我们关注一张图片，我们往往无需看清楚全部内容而仅将注意力集中在重点部分即可。而在自然语言处理领域，我们往往也可以通过将重点注意力集中在一个或几个 token，从而取得更高效高质的计算效果。
注意力机制有三个核心变量：Query（查询值）、Key（键值）和 Value（真值）。我们可以通过一个案例来理解每一个变量所代表的含义。例如，当我们有一篇新闻报道，我们想要找到这个报道的时间，那么，我们的 Query 可以是类似于“时间”、“日期”一类的向量（为了便于理解，此处使用文本来表示，但其实际是稠密的向量），Key 和 Value 会是整个文本。通过对 Query 和 Key 进行运算我们可以得到一个权重，这个权重其实反映了从 Query 出发，对文本每一个 token 应该分布的注意力相对大小。通过把权重和 Value 进行运算，得到的最后结果就是从 Query 出发计算整个文本注意力得到的结果。
具体而言，注意力机制的特点是通过计算 Query 与Key的相关性为真值加权求和，从而拟合序列中每个词同其他词的相关关系。


